{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genomics example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simulated data was as follows:\n",
    "- 1/4 sequences with 1-3 instances of a GATA_disc1 motif embedded (see http://compbio.mit.edu/encode-motifs/ for the PWM); these were labelled 1,0,0\n",
    "- 1/4 sequences with 1-3 instances of a TAL1_known1 motif embedded; these were labelled 0,1,0\n",
    "- 1/4 sequences with BOTH 1-3 instances of a GATA_disc1 motif AND 1-3 instances of a TAL1_known1 motif; these were labelled 1,1,1\n",
    "- 1/4 sequences with no motif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain data and keras model\n",
    "\n",
    "We will download genomic data and model\n",
    "\n",
    "### Download the data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File sequences.simdata.gz exists already\r\n",
      "File record_5_model_PQzyq_modelJson.json exists already\r\n",
      "File record_5_model_PQzyq_modelWeights.h5 exists already\r\n",
      "File test.txt.gz exists already\r\n"
     ]
    }
   ],
   "source": [
    "!./grab_model_and_data.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Read in and one-hot encode the data\n",
    "\n",
    "The simdna package is needed for reading the data; install it if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import simdna\n",
    "except ImportError, e:\n",
    "    print(\"installing simdna package\")\n",
    "    !pip install -e \"git://github.com/kundajelab/simdna.git@0.4.0#egg=simdna\"\n",
    "    print(\"\\n******************************************************************************\")\n",
    "    print(\"RESTART THE JUPYTER KERNEL TO PICK UP ON THE INSTALLATION!!!\")\n",
    "    print(\"******************************************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import simdna.synthetic as synthetic\n",
    "reload(synthetic)\n",
    "reload(synthetic.core)\n",
    "import gzip\n",
    "data_filename = \"sequences.simdata.gz\"\n",
    "\n",
    "#read in the data in the testing set\n",
    "test_ids_fh = gzip.open(\"test.txt.gz\",\"rb\")\n",
    "ids_to_load = [x.rstrip(\"\\n\") for x in test_ids_fh]\n",
    "data = synthetic.read_simdata_file(data_filename, ids_to_load=ids_to_load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encode the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#this model was trained on data one-hot encoded as a 2d image, with the row-axis being the axis\n",
    "#for one-hot encoding.\n",
    "def one_hot_encode_along_row_axis(sequence):\n",
    "    #theano dim ordering, uses row axis for one-hot\n",
    "    to_return = np.zeros((1,4,len(sequence)), dtype=np.int8)\n",
    "    seq_to_one_hot_fill_in_array(zeros_array=to_return[0],\n",
    "                                 sequence=sequence, one_hot_axis=0)\n",
    "    return to_return\n",
    "\n",
    "def seq_to_one_hot_fill_in_array(zeros_array, sequence, one_hot_axis):\n",
    "    assert one_hot_axis==0 or one_hot_axis==1\n",
    "    if (one_hot_axis==0):\n",
    "        assert zeros_array.shape[1] == len(sequence)\n",
    "    elif (one_hot_axis==1): \n",
    "        assert zeros_array.shape[0] == len(sequence)\n",
    "    #zeros_array should be an array of dim 4xlen(sequence), filled with zeros.\n",
    "    #will mutate zeros_array\n",
    "    for (i,char) in enumerate(sequence):\n",
    "        if (char==\"A\" or char==\"a\"):\n",
    "            char_idx = 0\n",
    "        elif (char==\"C\" or char==\"c\"):\n",
    "            char_idx = 1\n",
    "        elif (char==\"G\" or char==\"g\"):\n",
    "            char_idx = 2\n",
    "        elif (char==\"T\" or char==\"t\"):\n",
    "            char_idx = 3\n",
    "        elif (char==\"N\" or char==\"n\"):\n",
    "            continue #leave that pos as all 0's\n",
    "        else:\n",
    "            raise RuntimeError(\"Unsupported character: \"+str(char))\n",
    "        if (one_hot_axis==0):\n",
    "            zeros_array[char_idx,i] = 1\n",
    "        elif (one_hot_axis==1):\n",
    "            zeros_array[i,char_idx] = 1\n",
    "            \n",
    "onehot_data = np.array([one_hot_encode_along_row_axis(seq) for seq in data.sequences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "WARNING:theano.sandbox.cuda:The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GT 750M (CNMeM is disabled, cuDNN 5005)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import deeplift\n",
    "import deeplift.conversion.keras_conversion as kc\n",
    "\n",
    "#load the keras model\n",
    "keras_model_weights = \"record_5_model_PQzyq_modelWeights.h5\"\n",
    "keras_model_json = \"record_5_model_PQzyq_modelJson.json\"\n",
    "\n",
    "keras_model = kc.load_keras_model(weights=keras_model_weights,\n",
    "                                  json=keras_model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the deeplift models\n",
    "\n",
    "### Model conversion\n",
    "\n",
    "Convert the keras models to deeplift models capable of computing importance scores using DeepLIFT (with 3 different variants: rescale on the conv layers and revealcancel on the fully-connected layers (the genomics default), rescale on all layers, and revealcancel on all layers), gradients and guided backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nonlinear_mxts_mode is set to: DeepLIFT_GenomicsDefault\n",
      "For layer 1 the preceding linear layer is 0 of type Conv2D;\n",
      "In accordance with nonlinear_mxts_mode=DeepLIFT_GenomicsDefault we are setting the NonlinearMxtsMode to Rescale\n",
      "For layer 3 the preceding linear layer is 2 of type Conv2D;\n",
      "In accordance with nonlinear_mxts_mode=DeepLIFT_GenomicsDefault we are setting the NonlinearMxtsMode to Rescale\n",
      "For layer 7 the preceding linear layer is 6 of type Dense;\n",
      "In accordance with nonlinear_mxts_modeDeepLIFT_GenomicsDefault we are setting the NonlinearMxtsMode to RevealCancel\n",
      "For layer 10 the preceding linear layer is 9 of type Dense;\n",
      "In accordance with nonlinear_mxts_modeDeepLIFT_GenomicsDefault we are setting the NonlinearMxtsMode to RevealCancel\n"
     ]
    }
   ],
   "source": [
    "from deeplift.blobs import NonlinearMxtsMode\n",
    "from collections import OrderedDict\n",
    "\n",
    "method_to_model = OrderedDict()\n",
    "for method_name, nonlinear_mxts_mode in [\n",
    "    #The genomics default = rescale on conv layers, revealcance on fully-connected\n",
    "    ('rescale_conv_revealcancel_fc', NonlinearMxtsMode.DeepLIFT_GenomicsDefault)]:\n",
    "    method_to_model[method_name] = kc.convert_sequential_model(\n",
    "        model=keras_model,\n",
    "        nonlinear_mxts_mode=nonlinear_mxts_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity checks\n",
    "\n",
    "To ensure that the conversion happend correctly, ensure that the models give identical predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum difference in predictions: 0.0\n"
     ]
    }
   ],
   "source": [
    "#make sure predictions are the same as the original model\n",
    "from deeplift.util import compile_func\n",
    "model_to_test = method_to_model['rescale_conv_revealcancel_fc']\n",
    "deeplift_prediction_func = compile_func([model_to_test.get_layers()[0].get_activation_vars()],\n",
    "                                         model_to_test.get_layers()[-1].get_activation_vars())\n",
    "original_model_predictions = keras_model.predict(onehot_data, batch_size=200)\n",
    "converted_model_predictions = deeplift.util.run_function_in_batches(\n",
    "                                input_data_list=[onehot_data],\n",
    "                                func=deeplift_prediction_func,\n",
    "                                batch_size=200,\n",
    "                                progress_update=None)\n",
    "print(\"maximum difference in predictions:\",np.max(np.array(converted_model_predictions)-np.array(original_model_predictions)))\n",
    "assert np.max(np.array(converted_model_predictions)-np.array(original_model_predictions)) < 10**-5\n",
    "predictions = converted_model_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute importance scores\n",
    "\n",
    "### Compile various scoring functions\n",
    "\n",
    "Using the deeplift models, we obtain the functions capable of computing the importance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling scoring functions\n",
      "Compiling scoring function for: rescale_conv_revealcancel_fc\n"
     ]
    }
   ],
   "source": [
    "print(\"Compiling scoring functions\")\n",
    "method_to_scoring_func = OrderedDict()\n",
    "for method,model in method_to_model.items():\n",
    "    print(\"Compiling scoring function for: \"+method)\n",
    "    method_to_scoring_func[method] = model.get_target_multipliers_func(find_scores_layer_idx=0,\n",
    "                                                                    target_layer_idx=-2)\n",
    "\n",
    "deeplift_func = method_to_model['rescale_conv_revealcancel_fc'].get_target_contribs_func(find_scores_layer_idx=0,\n",
    "                                                                                               target_layer_idx=-2)\n",
    "multipliers_func = method_to_model['rescale_conv_revealcancel_fc'].get_target_multipliers_func(find_scores_layer_idx=0,\n",
    "                                                                                               target_layer_idx=-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Call scoring functions on the data\n",
    "\n",
    "In the cell below, a reference representing 40\\% GC content is used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using multiple shuffled references\n",
    "\n",
    "As an alternative to using a flat reference based on GC content (which can sometimes produce artefacts), we propose averaging the scores produced using mutliple references which are produced by shuffling the original sequence. We find in practice that this can give more robust results. Not that in general, the optimal choice of reference is an area of active research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 reference seqs generated\n",
      "2000 reference seqs generated\n",
      "3000 reference seqs generated\n",
      "4000 reference seqs generated\n",
      "5000 reference seqs generated\n",
      "6000 reference seqs generated\n",
      "7000 reference seqs generated\n",
      "8000 reference seqs generated\n",
      "One hot encoding sequences...\n",
      "One hot encoding done...\n",
      "Done 0\n",
      "Done 1000\n",
      "Done 2000\n",
      "Done 3000\n",
      "Done 4000\n",
      "Done 5000\n",
      "Done 6000\n",
      "Done 7000\n",
      "1000 reference seqs generated\n",
      "2000 reference seqs generated\n",
      "3000 reference seqs generated\n",
      "4000 reference seqs generated\n",
      "5000 reference seqs generated\n",
      "6000 reference seqs generated\n",
      "7000 reference seqs generated\n",
      "8000 reference seqs generated\n",
      "One hot encoding sequences...\n",
      "One hot encoding done...\n",
      "Done 0\n",
      "Done 1000\n",
      "Done 2000\n",
      "Done 3000\n",
      "Done 4000\n",
      "Done 5000\n",
      "Done 6000\n",
      "Done 7000\n"
     ]
    }
   ],
   "source": [
    "reload(deeplift.util)\n",
    "from deeplift.util import get_shuffle_seq_ref_function\n",
    "#from deeplift.util import randomly_shuffle_seq\n",
    "from deeplift.dinuc_shuffle import dinuc_shuffle #function to do a dinucleotide shuffle\n",
    "\n",
    "rescale_conv_revealcancel_fc_multipliers_many_refs_func = get_shuffle_seq_ref_function(\n",
    "    #score_computation_function is the original function to compute scores\n",
    "    score_computation_function=multipliers_func,\n",
    "    #shuffle_func is the function that shuffles the sequence\n",
    "    #technically, given the background of this simulation, randomly_shuffle_seq\n",
    "    #makes more sense. However, on real data, a dinuc shuffle is advisable due to\n",
    "    #the strong bias against CG dinucleotides\n",
    "    shuffle_func=dinuc_shuffle,\n",
    "    one_hot_func=lambda x: np.array([one_hot_encode_along_row_axis(seq) for seq in x]))\n",
    "\n",
    "rescale_conv_revealcancel_fc_many_refs_func = get_shuffle_seq_ref_function(\n",
    "    score_computation_function=deeplift_func,\n",
    "    shuffle_func=dinuc_shuffle,\n",
    "    one_hot_func=lambda x: np.array([one_hot_encode_along_row_axis(seq) for seq in x]))\n",
    "\n",
    "num_refs_per_seq=10 #number of references to generate per sequence\n",
    "multipliers_to_save = np.squeeze(rescale_conv_revealcancel_fc_multipliers_many_refs_func(\n",
    "                    task_idx=0,\n",
    "                    input_data_sequences=data.sequences,\n",
    "                    num_refs_per_seq=num_refs_per_seq,\n",
    "                    batch_size=200,\n",
    "                    progress_update=1000,\n",
    "                ))\n",
    "deeplift_scores_to_save = np.squeeze(np.sum(rescale_conv_revealcancel_fc_many_refs_func(\n",
    "            task_idx=0,\n",
    "            input_data_sequences=data.sequences,\n",
    "            num_refs_per_seq=num_refs_per_seq,\n",
    "            batch_size=200,\n",
    "            progress_update=1000,\n",
    "        ),axis=2)[:,:,None,:]*onehot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 200, 4)\n",
      "(800, 200, 4)\n"
     ]
    }
   ],
   "source": [
    "print(multipliers_to_save.transpose(0,2,1).shape)\n",
    "print(deeplift_scores_to_save.transpose(0,2,1).shape)\n",
    "\n",
    "#filter for cases where task 0 == 1, save matrix\n",
    "np.save(\"talgata_task0_positives_multipliers.npy\",\n",
    "        np.compress(condition=data.labels[:,0],\n",
    "                    a=multipliers_to_save.transpose(0,2,1), axis=0))\n",
    "\n",
    "np.save(\"talgata_task0_positives_scores.npy\",\n",
    "        np.compress(condition=data.labels[:,0],\n",
    "                    a=deeplift_scores_to_save.transpose(0,2,1), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
